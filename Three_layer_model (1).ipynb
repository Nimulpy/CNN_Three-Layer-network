{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP3Lk-7ewE_E"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Mounting Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define paths for training and validation data\n",
        "train_dir = '/content/drive/My Drive/Pneumonia Detection/train'\n",
        "val_dir = '/content/drive/My Drive/Pneumonia Detection/val'\n",
        "\n",
        "# Defining ImageDataGenerator for augmenting the training data\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Defining ImageDataGenerator for validating the validation data\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setting up the training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Setting up the validation data generator\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Creating the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding the first convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Adding the second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Adding the third convolutional layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening the output from the last convolutional layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Adding the first dense layer with 512 units and dropout\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Adding the second dense layer with 256 units and dropout\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Adding the output layer with sigmoid activation\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiling the model with binary crossentropy loss and Adam optimizer\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE)\n",
        "\n",
        "# Evaluating the model\n",
        "_, accuracy = model.evaluate(val_generator, steps=val_generator.samples // BATCH_SIZE)\n",
        "print('Accuracy on validation set:', accuracy)\n"
      ]
    }
  ]
}